Task 6: K-Nearest Neighbors (KNN) Classification
Objective:
Implement and understand K-Nearest Neighbors (KNN) for solving classification problems.

Tools Used:

Scikit-learn

Pandas

Matplotlib

Seaborn

ðŸ“Œ Steps Performed
Loaded the Iris Dataset from Kaggle.

Normalized features for distance-based learning.

Used KNeighborsClassifier from sklearn to train the model.

Experimented with different values of K to find the optimal number of neighbors.

Evaluated the model using accuracy score and confusion matrix.

Visualized decision boundaries for different classes.

ðŸ“Š Dataset
Iris Dataset

Features: Sepal length, sepal width, petal length, petal width.

Target: Species (Setosa, Versicolor, Virginica).

âœ… Key Learnings
KNN is an instance-based learning algorithm that makes predictions based on nearest neighbors.

Feature scaling is crucial for distance-based algorithms.

Optimal K is chosen by balancing bias and variance.

Decision boundaries help understand how KNN classifies regions in the feature space.

ðŸ“‚ Files in this folder
knn_classification.ipynb â†’ Notebook with data preprocessing, KNN implementation, evaluation, and visualizations.

